{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Reference: Heng CherKeng https://storage.googleapis.com/kaggle-forum-message-attachments/345216/9642/train.py\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import datetime\n",
    "from sklearn.neighbors import KDTree\n",
    "import collections as coll\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time as time\n",
    "from keras import models\n",
    "from keras.models import load_model\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.layers import Masking\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = '../../../../kaggle-trackml/input/train_1'\n",
    "TRAIN_NUMPY = '../../../../kaggle-trackml/input/train_numpy'\n",
    "TRAIN_SAMPLE_DATA = '../../../../kaggle-trackml/input/train_100_events'\n",
    "DBSCAN_DATA = '../../../../kaggle-trackml/src/r0_fast'\n",
    "GPU = 0 # 0 is default (usually uses 1 GPU if available, if > 1 then configures multi-gpu model)\n",
    "# Pass in saved model file name to retrain if desired, i.e \"2018-11-16-01-13-33.h5\".\n",
    "# If set to None, a new model will be built (if TRAIN_MODEL is True).\n",
    "LOAD_MODEL_NAME = None\n",
    "TRAIN_MODEL = False # If TRAIN_MODEL is True, model is automatically saved after training\n",
    "VISUALIZE_RESULTS = False\n",
    "predict_model_names = ['1024-512-epoch-256-mae.h5', '2048-epoch-400-mae.h5', '1024-epoch-220-mae.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpu():\n",
    "    found_gpus = K.tensorflow_backend._get_available_gpus()\n",
    "    print(\"Found GPUs: {}\".format(found_gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_train_history(history, metric='acc', metric_ylabel='Accuracy', metric_title='Training accuracy', draw_val=True, figsize=(12,5)):\n",
    "    \"\"\"Make plots of training and validation losses and accuracies\"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    # Plot loss\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.epoch, history.history['loss'], label='Training set')\n",
    "    if draw_val:\n",
    "        plt.plot(history.epoch, history.history['val_loss'], label='Validation set')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.epoch, history.history[metric], label='Training set')\n",
    "    if draw_val:\n",
    "        plt.plot(history.epoch, history.history[\"val_\" + metric], label='Validation set')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_ylabel)\n",
    "    plt.ylim((0, 1))\n",
    "    plt.title(metric_title)\n",
    "    plt.legend(loc=0)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "    \n",
    "def draw_prediction(truth, predict, start=0, end=1):\n",
    "    \"\"\"Visualize ground truth and predicted tracks.\n",
    "    Ground truth tracks will be in colour, and predicted tracks in grey.\"\"\"\n",
    "    fig1 = plt.figure(figsize=(12,12))\n",
    "    ax1  = fig1.add_subplot(111, projection='3d')\n",
    "    fig1.patch.set_facecolor('white')\n",
    "    ax1.set_xlabel('x', fontsize=16)\n",
    "    ax1.set_ylabel('y', fontsize=16)\n",
    "    ax1.set_zlabel('z', fontsize=16)\n",
    "\n",
    "    fig2 = plt.figure(figsize=(12,12))\n",
    "    ax2  = fig2.add_subplot(111, projection='3d')\n",
    "    fig2.patch.set_facecolor('white')\n",
    "    ax2.set_xlabel('Î¦', fontsize=16)\n",
    "    ax2.set_ylabel('r', fontsize=16)\n",
    "    ax2.set_zlabel('z', fontsize=16)\n",
    "\n",
    "    for n in range(start,end,1):\n",
    "        a, r, z, v = truth[n].T\n",
    "        x = r*1000*np.cos(a)\n",
    "        y = r*1000*np.sin(a)\n",
    "        z = z*3000\n",
    "        \n",
    "        ea, er, ez, ev = predict[n].T\n",
    "        ex = er*1000*np.cos(ea)\n",
    "        ey = er*1000*np.sin(ea)\n",
    "        ez = ez*3000\n",
    "        \n",
    "        color = np.random.uniform(0,1,3)\n",
    "        ax1.plot(ex,ey,ez,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax1.plot(x,y,z,'.-',color = color, markersize=5)\n",
    "        \n",
    "        ax2.plot(ea,er,ez,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax2.plot(a,r,z,'.-',color = color, markersize=5)\n",
    "        #if n==50: plt.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse (y_true, y_pred):\n",
    "    \"\"\"Implementation of root-mean-squared error, as an alternative loss function.\"\"\"\n",
    "    return K.sqrt(K.mean(K.square(y_pred -y_true), axis=-1))\n",
    "\n",
    "def compile_model(model, loss, optimizer, metrics):\n",
    "    \"\"\"Compile model. If multiple GPUs are desired (GPU > 0), a multi-GPU parallel model will be compiled.\"\"\"\n",
    "    if GPU > 0:\n",
    "        gpu_model = multi_gpu_model(model, GPU)\n",
    "    else:\n",
    "        gpu_model = model\n",
    " \n",
    "    gpu_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    " \n",
    "    return model, gpu_model\n",
    "\n",
    "def load_existing_model(filename, loss='mae', optimizer='Nadam', metrics=['accuracy', 'mse', 'mape']):\n",
    "    \"\"\"Load an existing model from disk and compile it. Can be used for inference, or further training.\"\"\"\n",
    "    model = load_model(filename)\n",
    "    return compile_model(model, loss, optimizer, metrics)\n",
    "\n",
    "def build_new_model(input_shape, output_shape, loss='mae', optimizer='Nadam', metrics=['accuracy', 'mse', 'mape']):\n",
    "    \"\"\"Build a new model to be trained.\"\"\"\n",
    "    # Alternative loss functions: 'mse', rmse, 'mape'\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Bidi LSTM 1024 neurons --> Bidi LSTM 512 neurons\n",
    "    #hidden_tmp = layers.Bidirectional(layers.LSTM(units=1024, return_sequences=True))(inputs)\n",
    "    #hidden = layers.Bidirectional(layers.LSTM(units=512, return_sequences=True))(hidden_tmp)\n",
    "    # LSTM 2048 neurons\n",
    "    # hidden = layers.LSTM(units=2048, return_sequences=True)(inputs)\n",
    "    # LSTM 1024 neurons\n",
    "    hidden = layers.LSTM(units=1024, return_sequences=True)(inputs)\n",
    "    outputs = layers.TimeDistributed(layers.Dense(output_shape[1], activation='linear'))(hidden)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return compile_model(model, loss, optimizer, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dbscan_seeds(labels, hits):\n",
    "    \"\"\"From dbscan input (labels), find seeds to input to LSTM.\"\"\"\n",
    "    df = add_features_to_df(hits, False)\n",
    "    # Add columns to help with eventual fitting\n",
    "    df['track_id'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "    df['predict_done'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "    df['actual_particle_id'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "\n",
    "    # Extract features that will be used by LSTM - a (phi), r, z, and z/r.\n",
    "    # Normalize so they are in the range of 0..1\n",
    "    a,r,z,z1 = df[['a', 'r', 'z', 'z1']].values.astype(np.float32).T\n",
    "    hit, p = df[['hit_id', 'particle_id']].values.astype(np.int64).T\n",
    "    incols1 = np.column_stack((a,r/1000, z/3000,z1/3))\n",
    "    incols2 = np.column_stack((hit, p))\n",
    "\n",
    "    all_tracks = np.unique(labels)\n",
    "    seed_tracks = []\n",
    "    truth_tracks = []\n",
    "\n",
    "    # Restrict to x,y,z values > 0 for now\n",
    "    positive_ix = np.where((df.x > 0) & (df.y > 0) & (df.z > 0))[0]\n",
    "    for track in all_tracks:\n",
    "        if track == 0: continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        track_hits = np.intersect1d(track_hits, positive_ix, assume_unique=True)\n",
    "        if len(track_hits) < 5: continue\n",
    "\n",
    "        # Sort on z - a cheap way of ordering a track based very loosely on 'time'\n",
    "        t = track_hits[np.argsort(z[track_hits])]\n",
    "        ti = incols1[t]\n",
    "        hit_p = incols2[t]\n",
    "\n",
    "        # Figure out the most likely truth particle ID for our track.\n",
    "        count_particle_ids = coll.Counter(hit_p[:5,1]).most_common(2)\n",
    "        seed_particle_id = count_particle_ids[0][0]\n",
    "        seed_particle_matches = count_particle_ids[0][1]\n",
    "        if seed_particle_id == 0 and count_particle_ids[0][1] < 5:\n",
    "            seed_particle_id = count_particle_ids[1][0]\n",
    "            seed_particle_matches = count_particle_ids[1][1]\n",
    "\n",
    "        truth_hits = np.where(df.particle_id == seed_particle_id)[0]\n",
    "        truth_hits = np.intersect1d(truth_hits, positive_ix, assume_unique=True)\n",
    "        # Our LSTM was trained on tracks of length >= 10, discard short tracks.\n",
    "        # If using all seeds regardless of truth track length, fitting would need\n",
    "        # to be changed to discard hits too far from any hit.\n",
    "        if len(truth_hits) < 10: continue\n",
    "        #    print(\"Truth track {} not long enough, actual length: {}\".format(seed_particle_id, len(truth_hits)))\n",
    "        tr = truth_hits[np.argsort(z[truth_hits])]\n",
    "        tri = incols1[tr]\n",
    "        trhit_p = incols2[tr]\n",
    "\n",
    "        seed_track = np.zeros((10,8))\n",
    "        truth_track = np.zeros((10,8))\n",
    "        for i in range(10):\n",
    "            truth_track[i]=([tri[i,0], tri[i,1], tri[i,2], tri[i,3], trhit_p[i,0], seed_particle_id, 0, 0])\n",
    "            if i < 5:\n",
    "                seed_track[i]=([ti[i,0], ti[i,1], ti[i,2], ti[i,3], hit_p[i,0], seed_particle_id, 0, hit_p[i,1]])\n",
    "            else:\n",
    "                seed_track[i]=([0,0,0,0,0, seed_particle_id, 0, 0])\n",
    "\n",
    "        seed_tracks.append(seed_track)\n",
    "        truth_tracks.append(truth_track)\n",
    "\n",
    "    seeds = np.array(seed_tracks)\n",
    "    truth = np.array(truth_tracks)\n",
    "\n",
    "    return df, seeds, truth\n",
    "\n",
    "def generate_dbscan_test_data(event_id, hits):\n",
    "    \"\"\"Generate seeds/test data from dbscan input.\"\"\"\n",
    "    helix1 = pd.read_csv(os.path.join(DBSCAN_DATA, \"event_{}_labels_train_helix42.csv\".format(event_id))).label.values\n",
    "    return find_dbscan_seeds(helix1, hits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_df(df, filter_negative=True):\n",
    "    \"\"\"Add feature columns to the event hits data frame.\"\"\"\n",
    "    df = df.copy()\n",
    "    if filter is True:\n",
    "        df = df.loc[ (df.x>0) & (df.y>0) & (df.z>0) ]\n",
    "        \n",
    "    df = df.assign(r   = np.sqrt( df.x**2 + df.y**2))\n",
    "    df = df.assign(a   = np.arctan2(df.y, df.x))\n",
    "    df = df.assign(z1 = df.z/df.r)\n",
    "    # Other possible features that can be evaluated?\n",
    "    #df = df.assign(d   = np.sqrt( df.x**2 + df.y**2 + df.z**2 ))\n",
    "    #df = df.assign(cosa= np.cos(df.a))\n",
    "    #df = df.assign(sina= np.sin(df.a))\n",
    "    #df = df.assign(phi = np.arctan2(df.z, df.r))\n",
    "    #df = df.assign(z2 = df.z/df.d)\n",
    "    #df = df.assign(z3 = np.log1p(np.absolute(df.z/df.r))*np.sign(df.z))\n",
    "    #df = df.assign(xr = df.x/df.r)\n",
    "    #df = df.assign(yr = df.y/df.r)\n",
    "    #df = df.assign(xd = df.x/df.d)\n",
    "    #df = df.assign(yd = df.y/df.d)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_one_event_data(event_id, path=TRAIN_DATA):\n",
    "    \"\"\"Load event data from disk.\"\"\"\n",
    "    # Only hits and truth data are currently used. May need to merge\n",
    "    # with the particles or cells data if you want to use those as features.\n",
    "    #particles = pd.read_csv(os.path.join(path, 'event%s-particles.csv'%event_id))\n",
    "    hits  = pd.read_csv(os.path.join(path, 'event%s-hits.csv' %event_id))\n",
    "    truth = pd.read_csv(os.path.join(path, 'event%s-truth.csv'%event_id))\n",
    "    #cells = pd.read_csv(os.path.join(path, 'event%s-cells.csv'%event_id))\n",
    "    \n",
    "    truth = truth.merge(hits,       on=['hit_id'],      how='left')\n",
    "    #truth = truth.merge(particles,  on=['particle_id'], how='left')\n",
    "    #truth = truth.merge(cells,      on=['hit_id'],      how='left')\n",
    "\n",
    "    #--------------------------------------------------------\n",
    "    df = truth\n",
    "    return (df)\n",
    "\n",
    "\n",
    "def generate_train_batch(df):\n",
    "    \"\"\"Generate training x and y values for a single event.\"\"\"\n",
    "    df = add_features_to_df(df)\n",
    "\n",
    "    a,r,z,z1 = df[['a', 'r', 'z', 'z1' ]].values.astype(np.float32).T\n",
    "    p = df['particle_id'].values.astype(np.int64)\n",
    "    \n",
    "    particle_ids = list(df.particle_id.unique())\n",
    "    num_particle_ids = len(particle_ids)\n",
    "\n",
    "    # Normalize inputs to a range from 0..1\n",
    "    incols  = np.column_stack((a,r/1000, z/3000,z1/3))\n",
    "    \n",
    "    tracks = []\n",
    "    \n",
    "    for particle_id in particle_ids:\n",
    "        if particle_id==0: continue\n",
    "        t = np.where(p==particle_id)[0]\n",
    "        # May need to sort by abs_z? or sort based on r?\n",
    "        t = t[np.argsort(z[t])]\n",
    "\n",
    "        # For simplicity for now, only train tracks >= 10 hits,\n",
    "        # and truncate to exactly 10 hits.\n",
    "        if len(t)<10: continue\n",
    "        track = incols[t[:10]]\n",
    "        tracks.append(track)\n",
    "\n",
    "    tracks = np.array(tracks)\n",
    "    incols = tracks[:,:5,:]\n",
    "    incols = np.pad(incols, ((0,0),(0,5), (0,0)), 'constant')\n",
    "   \n",
    "    truth  = tracks[:,:,:]\n",
    "\n",
    "    return (df, incols, truth)\n",
    "\n",
    "\n",
    "def batch_generator(invals, truth, batch_size):\n",
    "    \"\"\"Return a single batch of training data.\"\"\"\n",
    "    index = 0\n",
    "    while True:\n",
    "        batch_features = invals[batch_size*index:batch_size*(index+1),:,:]\n",
    "        batch_labels = truth[batch_size*index:batch_size*(index+1),:,:]\n",
    "        index = index+1\n",
    "        if batch_size*index > invals.shape[0]:\n",
    "            index=0\n",
    "      \n",
    "        yield batch_features, batch_labels\n",
    "\n",
    "def generate_multiple_event_data(skip=0, nevents=10):\n",
    "    \"\"\"Generate training x and y data for multiple events\"\"\"\n",
    "    start = 1000\n",
    "    invals = None\n",
    "    truth = None\n",
    "    for i in range(nevents):\n",
    "        try:\n",
    "            df = load_one_event_data('00000' + \"{:04}\".format(start+skip+i))\n",
    "            df, input_tmp, truth_tmp = generate_train_batch(df)\n",
    "            if i==0:\n",
    "                invals = input_tmp\n",
    "                truth = truth_tmp\n",
    "            else:\n",
    "                invals = np.concatenate([invals, input_tmp])\n",
    "                truth = np.concatenate([truth, truth_tmp])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return invals, truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_generate_multiple_event_data(skip=0, nevents=10):\n",
    "    \"\"\"Generating training data for events takes a very long time.\n",
    "    Use pre-generated numpy data for the specified events if possible (this is very fast).\n",
    "    Otherwise, generate the training data, and save it to disk for next time.\"\"\"\n",
    "    print(\"Generating or loading x_train, y_train with skip {:04} and nevents {:04}\".format(skip, nevents))\n",
    "    x_train_file = os.path.join(TRAIN_NUMPY, \"event_skip_{:04}_nevents_{:04}\".format(skip, nevents)+'_x_train.npy')\n",
    "    y_train_file = os.path.join(TRAIN_NUMPY, \"event_skip_{:04}_nevents_{:04}\".format(skip, nevents)+'_y_train.npy')\n",
    "\n",
    "    if os.path.exists(x_train_file):\n",
    "        x_train = np.load(x_train_file)\n",
    "        y_train = np.load(y_train_file)\n",
    "    else:\n",
    "        # This may take 1+ hour, once it's finished, save processed events as a numpy\n",
    "        # array, which can be loaded in a few milliseconds\n",
    "        print('Generating event data, this will take a long time...')\n",
    "        x_train, y_train = generate_multiple_event_data(skip=skip, nevents=nevents)\n",
    "        np.save(x_train_file, x_train)\n",
    "        np.save(y_train_file, y_train)\n",
    "\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data_for_fitting(df):\n",
    "    \"\"\"Generate test data for inference and fitting.\"\"\"\n",
    "    df = add_features_to_df(df, False)\n",
    "    # Generate any columns needed for fitting.\n",
    "    df['track_id'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "    df['predict_done'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "    df['actual_particle_id'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "\n",
    "    dfp = df.loc[ (df.x>0) & (df.y>0) & (df.z>0) ]\n",
    "    a, r, z, z1, hit, pid, pdone, pactual = dfp[['a', 'r', 'z', 'z1', 'hit_id', 'particle_id', 'predict_done', 'actual_particle_id' ]].values.astype(np.float32).T\n",
    "    p = dfp['particle_id'].values.astype(np.int64)\n",
    "    \n",
    "    particle_ids = list(dfp.particle_id.unique())\n",
    "    num_particle_ids = len(particle_ids)\n",
    "\n",
    "    incols  = np.column_stack((a,r/1000, z/3000,z1/3, hit, pid, pdone, pactual))    \n",
    "    tracks = []\n",
    "\n",
    "    for particle_id in particle_ids:\n",
    "        if particle_id==0: continue\n",
    "        t = np.where(p==particle_id)[0]\n",
    "        t = t[np.argsort(z[t])]\n",
    "\n",
    "        if len(t)<10: continue\n",
    "        track = incols[t[:10]]\n",
    "        track[:5,7] = particle_id\n",
    "        tracks.append(track)\n",
    "\n",
    "    tracks = np.array(tracks)\n",
    "    incols = np.copy(tracks[:,:,:])\n",
    "    # Clear hits 5-10, these will be predicted by the LSTM\n",
    "    incols[:,5:,0:4] = 0\n",
    "   \n",
    "    truth  = tracks[:,:,:]\n",
    "\n",
    "    return (df, incols, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predictions(hits, preds, verbose=False):\n",
    "    \"\"\"\n",
    "    Assign predicted LSTM hits to nearest un-assigned hit.\n",
    "    \"\"\"\n",
    "    def do_one_assignment_round(hits, preds, orig_preds):\n",
    "        labels = hits.track_id.values\n",
    "        distances = np.zeros((len(labels)), dtype=float)\n",
    "        predi = np.zeros((len(labels)), dtype=int)\n",
    "        predj = np.zeros((len(labels)), dtype=int)\n",
    "        #count_free_hits = len(np.where(labels == 0)[0])\n",
    "        #print(\"free hits available this round: {}\".format(count_free_hits))\n",
    "\n",
    "        # Note - df is the list of all available hits not yet assigned to\n",
    "        # a track. We need to fit our predictions to these hits.\n",
    "        df = hits.loc[(hits.track_id == 0)]\n",
    "        hit_ids = df.hit_id.values\n",
    "\n",
    "        # We have a (phi), r/1000, z/3000, and (z/r)/3.\n",
    "        # Manhattan distance seems to provide better fitting accuracy than\n",
    "        # euclidean - about a 0.7% improvement. It's unclear why manhattan\n",
    "        # would perform noticeably better than euclidean.\n",
    "        a, rn, zn, zrn, pid = df[['a', 'rn', 'zn', 'zrn', 'particle_id']].values.T\n",
    "        tree = KDTree(np.column_stack([a, rn, zn, zrn]), metric='manhattan')\n",
    "\n",
    "        num_left_to_assign = 0\n",
    "        # For each predicted track\n",
    "        for i in range(preds.shape[0]):\n",
    "            # For each predicted hit in that track\n",
    "            truth_particle = preds[i,0,5]\n",
    "            # hits 0-4 are the input seeds, already assigned\n",
    "            for j in range(5, 10):\n",
    "                # If we've already assigned this prediction to a hit in\n",
    "                # a previous round, skip it.\n",
    "                if preds[i,j,6] != 0: continue\n",
    "\n",
    "                # Find nearest neighbour to LSTM-predicted hits\n",
    "                ga = preds[i,j,0]\n",
    "                grn = preds[i,j,1]\n",
    "                gzn = preds[i,j,2]\n",
    "                gzrn = preds[i,j,3]\n",
    "                (nearest_dist, nearest_idx) = tree.query([[ga, grn, gzn, gzrn]], k=1)\n",
    "                nearest_dist = np.concatenate(nearest_dist)\n",
    "                nd0 = nearest_dist[0]\n",
    "                nearest_idx = np.concatenate(nearest_idx)\n",
    "                nidx0 = nearest_idx[0]\n",
    "                hit_id = hit_ids[nidx0]\n",
    "                gidx0 = hit_id - 1\n",
    "\n",
    "                #print(\"Nearest hit id is: {}, distance: {}\".format(hit_id, nd0))\n",
    "\n",
    "                # If the nearest hit is not assigned, or if this predicted hit is closer\n",
    "                # than any other predicted hits, assign this hit to us. Record our distance\n",
    "                # to the hit, in case later predicted hits in this batch end up being closer.\n",
    "                hits_index = gidx0\n",
    "                if (labels[hits_index] == 0) or (nd0 < distances[hits_index]):\n",
    "                    if (labels[hits_index] != 0):\n",
    "                        # We stole someone else's prediction since we are closer to\n",
    "                        # that hit, so clear the 'prediction_done' flag\n",
    "                        far_i = predi[hits_index]\n",
    "                        far_j = predj[hits_index]\n",
    "                        preds[far_i,far_j,:] = orig_preds[far_i,far_j,:]\n",
    "                        num_left_to_assign = num_left_to_assign + 1\n",
    "                    # Tentatively assign this hit\n",
    "                    preds[i,j,6] = 1\n",
    "                    # And record what the actual particle/track ID was\n",
    "                    preds[i,j,7] = pid[nidx0]\n",
    "                    # And remember the fitted a/rn/zn/zrn values so we can visualize\n",
    "                    preds[i,j,0] = a[nidx0]\n",
    "                    preds[i,j,1] = rn[nidx0]\n",
    "                    preds[i,j,2] = zn[nidx0]\n",
    "                    preds[i,j,3] = zrn[nidx0]\n",
    "\n",
    "                    # And record our track and the distance between the projected hit\n",
    "                    # and the actual hit. This distance is used in case a different\n",
    "                    # projected hit has this same target as its nearest neighbour.\n",
    "                    # The target hit will be assigned to the closest one.\n",
    "                    labels[hits_index] = truth_particle\n",
    "                    distances[hits_index] = nd0\n",
    "                    predi[hits_index] = i\n",
    "                    predj[hits_index] = j\n",
    "                else:\n",
    "                    num_left_to_assign = num_left_to_assign + 1\n",
    "\n",
    "        # Update the data frame with the new track data from this round of fitting.\n",
    "        hits['track_id'] = labels.tolist()\n",
    "        return (hits, preds, num_left_to_assign)\n",
    "\n",
    "    # Main function starts here.\n",
    "    # Add any missing fitting features to the data frame. Most are already\n",
    "    # set up when we loaded the test data, mainly just need to normalize.\n",
    "    #hits['z_abs'] = hits.z.abs()\n",
    "    #hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['rn'] = hits['r'] / 1000\n",
    "    #hits['a'] = np.arctan2(hits.y.values, hits.x.values)\n",
    "    #hits['zr'] = hits['z'] / hits['r']\n",
    "    hits['zrn'] = hits['z1'] / 3\n",
    "    hits['zn'] = hits['z'] / 3000\n",
    "\n",
    "\n",
    "    # Initialize tracks using our initial 5 seed hits, and using the\n",
    "    # truth particle ID as the track ID\n",
    "    labels = hits.track_id.values\n",
    "    for i in range(preds.shape[0]):\n",
    "        truth_particle_id = preds[i,0,5]\n",
    "        # First 5 hits are known, update track_id with ground truth particle ID\n",
    "        for j in range(5):\n",
    "            hit_id = int(preds[i,j,4])\n",
    "            hits_index = hit_id - 1\n",
    "            labels[hits_index] = truth_particle_id\n",
    "    hits['track_id'] = labels.tolist()\n",
    "\n",
    "    num_left_to_assign = preds.shape[0]*5 # 5 hits to assign per predicted track\n",
    "    num_loops = 0\n",
    "    # Backup current predictions to restore a/rn/zn/zrn values if\n",
    "    # another hit ends up being closer\n",
    "    orig_preds = np.copy(preds)\n",
    "    while (num_left_to_assign > 0) and (num_loops < 10):\n",
    "        (hits, preds, num_left_to_assign) = do_one_assignment_round(hits, preds, orig_preds)\n",
    "        num_loops = num_loops + 1\n",
    "        if verbose:\n",
    "            print(\"Num left to assign after round {}: {}\".format(num_loops, num_left_to_assign))\n",
    "\n",
    "    return (hits, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fit_accuracy(preds, verbose=False):\n",
    "    \"\"\"Calculate our inference+fitting accuracy.\"\"\"\n",
    "    total_correct = 0\n",
    "    total_incorrect = 0\n",
    "    tracks_right = np.zeros((6), dtype=int)\n",
    "    seeds_right = np.zeros((6), dtype=int)\n",
    "    total_seed_correct = 0\n",
    "    total_seed_incorrect = 0\n",
    "    for i in range(preds.shape[0]):\n",
    "        track_right = 0\n",
    "        seed_right = 0\n",
    "        # Calculate seed accuracy\n",
    "        for j in range(5):\n",
    "            #print(\"Truth: {}, fit: {}\".format(preds[i,j,5], preds[i,j,7]))\n",
    "            if preds[i][j][5] != preds[i][j][7]:\n",
    "                total_seed_incorrect = total_seed_incorrect + 1\n",
    "            else:\n",
    "                total_seed_correct = total_seed_correct + 1\n",
    "                seed_right = seed_right + 1\n",
    "        seeds_right[seed_right] = seeds_right[seed_right] + 1\n",
    "\n",
    "        # Calculate inference+fit accuracy\n",
    "        for j in range(5, 10):\n",
    "            #print(\"Truth: {}, fit: {}\".format(preds[i,j,5], preds[i,j,7]))\n",
    "            if preds[i][j][5] != preds[i][j][7]:\n",
    "                total_incorrect = total_incorrect + 1\n",
    "            else:\n",
    "                total_correct = total_correct + 1\n",
    "                track_right = track_right + 1\n",
    "        tracks_right[track_right] = tracks_right[track_right] + 1\n",
    "\n",
    "    accuracy = total_correct / (total_correct + total_incorrect)\n",
    "    seed_accuracy = total_seed_correct / (total_seed_correct + total_seed_incorrect)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Total correct:   {}\".format(total_correct))\n",
    "        print(\"Total incorrect: {}\".format(total_incorrect))\n",
    "        print(\"Total accuracy:  {}\".format(accuracy))\n",
    "        print(\"Total correct predictions per track (0-5): {}\".format(tracks_right))\n",
    "\n",
    "        print(\"Total Seed correct:   {}\".format(total_seed_correct))\n",
    "        print(\"Total Seed incorrect: {}\".format(total_seed_incorrect))\n",
    "        print(\"Total Seed accuracy:  {}\".format(seed_accuracy))\n",
    "        print(\"Total Seed per track (0-5): {}\".format(seeds_right))\n",
    "\n",
    "    return accuracy, tracks_right, seed_accuracy, seeds_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions(model_names, dbscan=False, first_event=9998, num_events=1, verbose=False):\n",
    "    \"\"\"Perform inference (using ensemble of provided models) and fitting for the specified events.\"\"\"\n",
    "\n",
    "    # Load all models we will use for inference+ensembling\n",
    "    start = time.time()\n",
    "    gpu_models = []\n",
    "    for i in range(len(model_names)):\n",
    "        _, gpu_modeli = load_existing_model(model_names[i])\n",
    "        gpu_models.append(gpu_modeli)\n",
    "    num_models = len(gpu_models)\n",
    "    elapsed = find_elapsed_time(start, \"Model load time ({} models): \".format(num_models))\n",
    "\n",
    "    avg_accuracy = 0.0\n",
    "    track_dists = np.zeros((6), dtype=int)\n",
    "    avg_seed_accuracy = 0.0\n",
    "    seed_dists = np.zeros((6), dtype=int)\n",
    "    inference_times = np.zeros((num_models), dtype=float)\n",
    "    total_fit_time = 0.0\n",
    "    total_tracks = 0\n",
    "\n",
    "    test_x = None\n",
    "    test_y = None\n",
    "    \n",
    "    for ev in range(num_events):\n",
    "\n",
    "        # Generate test data for this event for inference+fitting\n",
    "        start = time.time()\n",
    "        event_id = first_event + ev\n",
    "        fit_df = load_one_event_data(\"00000{}\".format(event_id))\n",
    "        if dbscan:\n",
    "            fit_df, pred_x, pred_truth = generate_dbscan_test_data(event_id, fit_df)\n",
    "        else:\n",
    "            fit_df, pred_x, pred_truth = generate_test_data_for_fitting(fit_df)\n",
    "        num_tracks = pred_truth.shape[0]\n",
    "        total_tracks = total_tracks + num_tracks\n",
    "        elapsed = find_elapsed_time(start, \"Event {} with {} tracks setup time: \".format(event_id, num_tracks))\n",
    "\n",
    "        # Accumulate raw predictions from each model\n",
    "        preds = np.copy(pred_x)\n",
    "        preds[:,5:,0:4] = 0\n",
    "        elapsed = 0\n",
    "        for i in range(num_models):\n",
    "            start = time.time()\n",
    "            pred_raw = gpu_models[i].predict(pred_x[:,:,0:4])\n",
    "            this_elapsed = find_elapsed_time(start, \"predict time: \", display_time=verbose)\n",
    "            preds[:,5:,0:4] = preds[:,5:,0:4] + pred_raw[:,5:,0:4]\n",
    "            elapsed = elapsed + this_elapsed\n",
    "            inference_times[i] = inference_times[i] + this_elapsed\n",
    "\n",
    "            if verbose:\n",
    "                test_loss, test_acc, test_mae, test_mape = gpu_models[i].evaluate(pred_x[:,:,0:4], pred_truth[:,:,0:4])\n",
    "                print(\"Loss: MSE: {:f}, MAE: {:f}, MAPE: {:f}\".format(test_loss, test_mae, test_mape))\n",
    "\n",
    "        print(\"Event {} inference time: {:f} (per track: {:f})\".format(event_id, elapsed, elapsed/num_tracks))\n",
    "\n",
    "        # Average accumulated predictions\n",
    "        # Note - the accumulation above could be enhanced with weights if desired, i.e. to\n",
    "        # give a higher weight to a more accurate model, while still allowing less accurate\n",
    "        # models to impact the final result slightly\n",
    "        preds[:,5:,0:4] = preds[:,5:,0:4] / num_models\n",
    "\n",
    "        # Set up predictions for kNN fitting - we only fit predictions 5-10.\n",
    "        start = time.time()\n",
    "        (fit_df, preds) = fit_predictions(fit_df, preds)\n",
    "        elapsed = find_elapsed_time(start, \"x\", display_time=False)\n",
    "        total_fit_time = total_fit_time + elapsed\n",
    "        print(\"Event {} fit time: {:f} (per track: {:f})\".format(event_id, elapsed, elapsed/num_tracks))\n",
    "\n",
    "        # Calculate our inference+fitting accuracy\n",
    "        (accuracy, dist, seed_accuracy, seed_dist) = calculate_fit_accuracy(preds, verbose=verbose)\n",
    "        print(\"Event {} accuracy: {:f}, distribution (0-5 correct): {}\".format(event_id, accuracy, dist))\n",
    "        print(\"Event {} seed accuracy: {:f}, distribution (0-5 correct): {}\".format(event_id, seed_accuracy, seed_dist))\n",
    "        avg_accuracy = avg_accuracy + accuracy\n",
    "        avg_seed_accuracy = avg_seed_accuracy + seed_accuracy\n",
    "        track_dists = track_dists + dist\n",
    "        seed_dists = seed_dists + seed_dist\n",
    "\n",
    "        test_x = np.copy(preds)\n",
    "        test_y = np.copy(pred_truth)\n",
    "        if VISUALIZE_RESULTS:\n",
    "            draw_prediction(pred_truth[:,:,0:4], preds[:,:,0:4], 270, 280)\n",
    "\n",
    "    if num_events > 1:\n",
    "        avg_accuracy = avg_accuracy / num_events\n",
    "        avg_seed_accuracy = avg_seed_accuracy / num_events\n",
    "        total_inference_time = np.sum(inference_times)\n",
    "        inference_time_event_avg = inference_times / num_events\n",
    "        inference_time_event_avg_sum = np.sum(inference_time_event_avg)\n",
    "        inference_time_track_avg = inference_times / total_tracks\n",
    "        inference_time_track_avg_sum = np.sum(inference_time_track_avg)\n",
    "        fit_time_avg = total_fit_time / num_events\n",
    "        tracks_avg = total_tracks / num_events\n",
    "        print(\"Total tracks predicted: {:d}, average per-event: {:d}\".format(total_tracks, int(tracks_avg)))\n",
    "        print(\"Total inference time: {:f}, per-model: {}\".format(total_inference_time, inference_times))\n",
    "        print(\"Average accuracy: {:f}, total distribution (0-5 correct): {}\".format(avg_accuracy, track_dists))\n",
    "        print(\"Average seed accuracy: {:f}, total seed distribution (0-5 correct): {}\".format(avg_seed_accuracy, seed_dists))\n",
    "        print(\"Average event model inference times: {:f}, {}\".format(inference_time_event_avg_sum, inference_time_event_avg))\n",
    "        print(\"Average track model inference times: {:f}, {}\".format(inference_time_track_avg_sum, inference_time_track_avg))\n",
    "        print(\"Total fit time: {:f}, average fit time: {:f}\".format(total_fit_time, fit_time_avg))\n",
    "        \n",
    "    return test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_elapsed_time(start, label, display_time=True):\n",
    "    elapsed = time.time()\n",
    "    elapsed = elapsed - start\n",
    "    if display_time:\n",
    "        print(\"{}{:f}\".format(label, elapsed))\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # For playing around just load a couple hundred events, for final LSTM\n",
    "    # training, all possible events should be used to minimize loss.\n",
    "    train_skip = 0\n",
    "    train_nevents = 200\n",
    "    val_skip = 0\n",
    "    val_nevents = 20\n",
    "    print('loading train data...')\n",
    "    invals, truth = load_or_generate_multiple_event_data(skip=train_skip, nevents=train_nevents)\n",
    "    print('Done!')\n",
    "\n",
    "    print('loading val data...')\n",
    "    val_input, val_truth = load_or_generate_multiple_event_data(skip=train_skip+train_nevents+val_skip, nevents=val_nevents)\n",
    "    print('Done!')\n",
    "\n",
    "    # Resume training from an existing model if provided, otherwise create a new model for training.\n",
    "    if LOAD_MODEL_NAME is not None:\n",
    "        model, gpu_model = load_existing_model(LOAD_MODEL_NAME)\n",
    "    else:\n",
    "        model, gpu_model = build_new_model(input_shape=(invals.shape[1],invals.shape[2]), output_shape=(invals.shape[1],invals.shape[2]))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    batch_size = 16384\n",
    "    num_epoch = 500\n",
    "    earlystopper = EarlyStopping(patience=20, verbose=0)\n",
    "    # The training data is small enough that it can all fit in memory, so a batch generator\n",
    "    # is not needed (yet). If training with all data (not just x,y,z > 0), we may need to\n",
    "    # use the batch generator.\n",
    "    #generator = batch_generator(invals, truth, batch_size)\n",
    "    #val_generator = batch_generator(val_input, val_truth, batch_size)\n",
    "    # Train the model\n",
    "    #history = gpu_model.fit_generator(generator, validation_data=val_generator,validation_steps=int(val_input.shape[0]/batch_size),\n",
    "    #                              steps_per_epoch=int(invals.shape[0]/batch_size), epochs=num_epoch,\n",
    "    #                              callbacks=[earlystopper],\n",
    "    #                              shuffle = False)\n",
    "    history = gpu_model.fit(x=invals, y=truth, validation_data=(val_input, val_truth), batch_size=batch_size,\n",
    "                            epochs=num_epoch, callbacks=[earlystopper], shuffle=False)\n",
    "\n",
    "    current_datetime_str = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    save_model_name = current_datetime_str + '.h5'\n",
    "    model.save(save_model_name)\n",
    "    predict_model_names.append(save_model_name)\n",
    "    print(\"End time: {}\".format(current_datetime_str))\n",
    "\n",
    "    if VISUALIZE_RESULTS:\n",
    "        draw_train_history(history, metric='mean_squared_error', metric_ylabel='Mean Squared Err', metric_title='Mean Squared Error', draw_val=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/trackml/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model load time (3 models): 11.916496\n",
      "Event 9998 with 646 tracks setup time: 0.500620\n",
      "Event 9998 inference time: 9.651171 (per track: 0.014940)\n",
      "Event 9998 fit time: 0.786522 (per track: 0.001218)\n",
      "Event 9998 accuracy: 0.875851, distribution (0-5 correct): [  2   7  26  57 171 383]\n",
      "Event 9998 seed accuracy: 1.000000, distribution (0-5 correct): [  0   0   0   0   0 646]\n"
     ]
    }
   ],
   "source": [
    "#predict_model_names = ['2048-epoch-400-mae.h5', '1024-epoch-400-mae.h5']\n",
    "# truth: 646 tracks, 86.0% ([  3  13  27  75 155 373])\n",
    "# dbscan: 452 tracks, 82.9% ([  9  11  35  51  90 256]), seeds 95.6% ([  0   1   4  20  43 384])\n",
    "# dbscan 100: Average accuracy: 0.803291, total distribution (0-5 correct): [ 1574  1580  3264  5182  8920 23300]\n",
    "# dbscan 100: Average seed accuracy: 0.957257, total seed distribution (0-5 correct): [    0    26   777  1619  3787 37611]\n",
    "# truth 100:  Average accuracy: 0.845796, total distribution (0-5 correct): [  190  1290  3792  8004 15838 34696]\n",
    "#predict_model_names = ['3072-epoch-400-mae.h5', '2048-epoch-400-mae.h5', '1024-epoch-400-mae.h5']\n",
    "# 3072: 82.8%, 3072+2048: 85.4%, 3072+2048+1024: 86.1%\n",
    "#predict_model_names = ['Bi1024-Bi512-epoch-120-mae.h5', '2048-epoch-400-mae.h5', '1024-epoch-400-mae.h5']\n",
    "# Bi: 83.1%, Bi+2048: 85.9%, Bi+2048+1024: 86.2%\n",
    "# truth: 646 tracks, 86.2% ([  2  10  32  66 167 369])\n",
    "# dbscan: 452 tracks, 83.2%, ([  9  11  32  56  83 261]), seeds 95.6% ([  0   1   4  20  43 384])\n",
    "#predict_model_names = ['1024-512-epoch-256-mae.h5', '2048-epoch-400-mae.h5', '1024-epoch-400-mae.h5']\n",
    "# truth: 646 tracks, 86.6% ([  2  10  36  61 153 384])\n",
    "# dbscan: 452 tracks, 83.4%, ([ 10   8  30  57  89 258]), seeds 95.6% ([  0   1   4  20  43 384])\n",
    "#predict_model_names = ['1024-512-epoch-256-mae.h5', '2048-epoch-400-mae.h5', '1024-epoch-220-mae.h5']\n",
    "# truth: 646 tracks, 86.9% ([  2   9  30  55 178 372])\n",
    "# dbscan: 452 tracks, 83.6% ([ 11   6  30  52  98 255]), seeds 95.6% ([  0   1   4  20  43 384])\n",
    "## manhattan distance: truth:  87.6% ([  2   7  26  56 171 384])\n",
    "## manhattan distance: dbscan: 83.9% ([ 11   7  27  52  96 259])\n",
    "#predict_model_names = ['1024-512-epoch-256-mae.h5', '2048-epoch-400-mae.h5', '1024-epoch-220-mae.h5', '256-10-512-10-1024-epoch-206-mae.h5']\n",
    "(test_x, test_y) = ensemble_predictions(predict_model_names, dbscan=False, first_event=9998, num_events=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#65-70 is good\n",
    "#95-100 is quite good\n",
    "#110-115 is good\n",
    "#125-130 is quite good\n",
    "#140-145 is quite, quite good\n",
    "#160-165 is quite, quite good\n",
    "#165-170 is quite, quite good\n",
    "#175-180 is quite good\n",
    "#245-250 are almost perfect predictions (not interesting enough?)\n",
    "##255-260 is quite good\n",
    "#265-270 is interesting - extremely good, but reversed (straight in xyz, curved in polar)\n",
    "#300-305 is quite, quite good\n",
    "#310-315 is quite, quite good\n",
    "#315-320 is quite, quite good\n",
    "if VISUALIZE_RESULTS:\n",
    "    draw_prediction(test_y[:,:,0:4], test_x[:,:,0:4], 0, 645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
