{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Reference: Heng CherKeng https://storage.googleapis.com/kaggle-forum-message-attachments/345216/9642/train.py\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import datetime\n",
    "from sklearn.neighbors import KDTree\n",
    "import collections as coll\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time as time\n",
    "from keras import models\n",
    "from keras.models import load_model\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.layers import Masking\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = '../../../../kaggle-trackml/input/train_1'\n",
    "TRAIN_NUMPY = '../../../../kaggle-trackml/input/train_numpy'\n",
    "TRAIN_SAMPLE_DATA = '../../../../kaggle-trackml/input/train_100_events'\n",
    "DBSCAN_DATA = '../../../../kaggle-trackml/src/r0_fast'\n",
    "GPU = 0 # 0 is default (usually uses 1 GPU if available, if > 1 then configures multi-gpu model)\n",
    "# Pass in saved model file name to retrain if desired, i.e \"2018-11-16-01-13-33.h5\".\n",
    "# If set to None, a new model will be built (if TRAIN_MODEL is True).\n",
    "LOAD_MODEL_NAME = None #\"1024-epoch-200.h5\"\n",
    "TRAIN_MODEL = False # Model is automatically saved after training\n",
    "VISUALIZE_RESULTS = False\n",
    "predict_model_names = ['2048-epoch-400-mae.h5', '1024-epoch-400-mae.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpu():\n",
    "    found_gpus = K.tensorflow_backend._get_available_gpus()\n",
    "    print(\"Found GPUs: {}\".format(found_gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_train_history(history, metric='acc', metric_ylabel='Accuracy', metric_title='Training accuracy', draw_val=True, figsize=(12,5)):\n",
    "    \"\"\"Make plots of training and validation losses and accuracies\"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    # Plot loss\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.epoch, history.history['loss'], label='Training set')\n",
    "    if draw_val:\n",
    "        plt.plot(history.epoch, history.history['val_loss'], label='Validation set')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.epoch, history.history[metric], label='Training set')\n",
    "    if draw_val:\n",
    "        plt.plot(history.epoch, history.history[\"val_\" + metric], label='Validation set')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_ylabel)\n",
    "    plt.ylim((0, 1))\n",
    "    plt.title(metric_title)\n",
    "    plt.legend(loc=0)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def draw_prediction_2d(truth, predict, start=0, end=1):\n",
    "    fig1 = plt.figure(figsize=(12,12))\n",
    "    ax1  = fig1.add_subplot(2,1,1)\n",
    "    ax1.set_xlabel('x', fontsize=16)\n",
    "    ax1.set_ylabel('y', fontsize=16)\n",
    "    \n",
    "    fig2 = plt.figure(figsize=(12,12))\n",
    "    ax2 = fig2.add_subplot(2,1,1)\n",
    "    ax2.set_xlabel('a', fontsize=16)\n",
    "    ax2.set_ylabel('r', fontsize=16)\n",
    "    \n",
    "    for n in range(start,end,1):\n",
    "        a, r, z = truth[n].T\n",
    "        x = r*np.cos(a)\n",
    "        y = r*np.sin(a)\n",
    "        ea, er, ez = predict[n].T\n",
    "        ex = er*np.cos(ea)\n",
    "        ey = er*np.sin(ea)\n",
    "        \n",
    "        color = np.random.uniform(0,1,3)\n",
    "        ax1.plot(ex,ey,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax1.plot(x,y,'.-',color = color, markersize=5)\n",
    "        \n",
    "        ax2.plot(ea,er,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax2.plot(a,r,'.-',color = color, markersize=5)\n",
    "\n",
    "    \n",
    "def draw_prediction(truth, predict, start=0, end=1):\n",
    "    fig1 = plt.figure(figsize=(12,12))\n",
    "    ax1  = fig1.add_subplot(111, projection='3d')\n",
    "    fig1.patch.set_facecolor('white')\n",
    "    ax1.set_xlabel('x', fontsize=16)\n",
    "    ax1.set_ylabel('y', fontsize=16)\n",
    "    ax1.set_zlabel('z', fontsize=16)\n",
    "\n",
    "    fig2 = plt.figure(figsize=(12,12))\n",
    "    ax2  = fig2.add_subplot(111, projection='3d')\n",
    "    fig2.patch.set_facecolor('white')\n",
    "    ax2.set_xlabel('a', fontsize=16)\n",
    "    ax2.set_ylabel('r', fontsize=16)\n",
    "    ax2.set_zlabel('z', fontsize=16)\n",
    "\n",
    "    for n in range(start,end,1):\n",
    "        a, r, z, v = truth[n].T\n",
    "        x = r*1000*np.cos(a)\n",
    "        y = r*1000*np.sin(a)\n",
    "        z = z*3000\n",
    "        \n",
    "        ea, er, ez, ev = predict[n].T\n",
    "        ex = er*1000*np.cos(ea)\n",
    "        ey = er*1000*np.sin(ea)\n",
    "        ez = ez*3000\n",
    "        \n",
    "        color = np.random.uniform(0,1,3)\n",
    "        ax1.plot(ex,ey,ez,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax1.plot(x,y,z,'.-',color = color, markersize=5)\n",
    "        \n",
    "        ax2.plot(ea,er,ez,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax2.plot(a,r,z,'.-',color = color, markersize=5)\n",
    "        if n==50: plt.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred -y_true), axis=-1))\n",
    "\n",
    "def compile_model(model, loss, optimizer, metrics):\n",
    "    if GPU > 0:\n",
    "        gpu_model = multi_gpu_model(model, GPU)\n",
    "    else:\n",
    "        gpu_model = model\n",
    " \n",
    "    gpu_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    " \n",
    "    return model, gpu_model\n",
    "\n",
    "def load_existing_model(filename, loss='mae', optimizer='Nadam', metrics=['accuracy', 'mse', 'mape']):\n",
    "    model = load_model(filename)\n",
    "    return compile_model(model, loss, optimizer, metrics)\n",
    "\n",
    "def build_new_model(input_shape, output_shape,\n",
    "                loss='mae', #loss='mse', rmse, 'mape'\n",
    "                optimizer='Nadam', metrics=['accuracy', 'mse', 'mape']):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    hidden = layers.LSTM(units=1024, return_sequences=True)(inputs)\n",
    "    #dropout = layers.Dropout(0.2)(hidden)\n",
    "    #hidden2 = layers.LSTM(units=24, return_sequences=True)(dropout)\n",
    "    outputs = layers.TimeDistributed(layers.Dense(output_shape[1], activation='linear'))(hidden)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return compile_model(model, loss, optimizer, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-80680aa892f1>, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-80680aa892f1>\"\u001b[0;36m, line \u001b[0;32m52\u001b[0m\n\u001b[0;31m    seed_track[i]=([ti[i,0], ti[i,1], ti[i,2], ti[i,3], hit_p[i,0], seed_particle_id, 0, hit_p[i,1])\u001b[0m\n\u001b[0m                                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def generate_dbscan_tracks(labels, hits):\n",
    "    df = generate_df(hits, False)\n",
    "    df['track_id'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "    df['predict_done'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "    df['actual_particle_id'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "\n",
    "    a,r,z,z1 = df[['a', 'r', 'z', 'z1']].values.astype(np.float32).T\n",
    "    hit, p = df[['hit_id', 'particle_id']].values.astype(np.int64).T\n",
    "    incols1 = np.column_stack((a,r/1000, z/3000,z1/3))\n",
    "    incols2 = np.column_stack((hit, p))\n",
    "\n",
    "    all_tracks = np.unique(labels)\n",
    "    seed_tracks = []\n",
    "    truth_tracks = []\n",
    "\n",
    "    # Restrict to x,y,z values > 0 for now\n",
    "    positive_ix = np.where((df.x > 0) & (df.y > 0) & (df.z > 0))[0]\n",
    "    for track in all_tracks:\n",
    "        if track == 0: continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        track_hits = np.intersect1d(track_hits, positive_ix, assume_unique=True)\n",
    "        if len(track_hits) < 5: continue\n",
    "\n",
    "        t = track_hits[np.argsort(z[track_hits])]\n",
    "        ti = incols1[t]\n",
    "        hit_p = incols2[t]\n",
    "\n",
    "        # Figure out the most likely truth particle ID for our track.\n",
    "        count_particle_ids = coll.Counter(hit_p[:5,1]).most_common(2)\n",
    "        seed_particle_id = count_particle_ids[0][0]\n",
    "        seed_particle_matches = count_particle_ids[0][1]\n",
    "        if seed_particle_id == 0 and count_particle_ids[0][1] < 5:\n",
    "            seed_particle_id = count_particle_ids[1][0]\n",
    "            seed_particle_matches = count_particle_ids[1][1]\n",
    "\n",
    "        # Un-comment to discard horrible seeds where <3 hits match\n",
    "        #if seed_particle_matches < 3: continue\n",
    "\n",
    "        truth_hits = np.where(df.particle_id == seed_particle_id)[0]\n",
    "        truth_hits = np.intersect1d(truth_hits, positive_ix, assume_unique=True)\n",
    "        if len(truth_hits) < 10: continue\n",
    "        #    print(\"Truth track {} not long enough, actual length: {}\".format(seed_particle_id, len(truth_hits)))\n",
    "        tr = truth_hits[np.argsort(z[truth_hits])]\n",
    "        tri = incols1[tr]\n",
    "        trhit_p = incols2[tr]\n",
    "\n",
    "        seed_track = np.zeros((10,8))\n",
    "        truth_track = np.zeros((10,8))\n",
    "        for i in range(10):\n",
    "            truth_track[i]=([tri[i,0], tri[i,1], tri[i,2], tri[i,3], trhit_p[i,0], seed_particle_id, 0, 0])\n",
    "            if i < 5:\n",
    "                seed_track[i]=([ti[i,0], ti[i,1], ti[i,2], ti[i,3], hit_p[i,0], seed_particle_id, 0, hit_p[i,1])\n",
    "            else:\n",
    "                seed_track[i]=([0,0,0,0,0, seed_particle_id, 0, 0])\n",
    "\n",
    "        seed_tracks.append(seed_track)\n",
    "        truth_tracks.append(truth_track)\n",
    "\n",
    "    seeds = np.array(seed_tracks)\n",
    "    truth = np.array(truth_tracks)\n",
    "\n",
    "    return df, seeds, truth\n",
    "\n",
    "def generate_dbscan_test_data(event_id, hits):\n",
    "    helix1 = pd.read_csv(os.path.join(DBSCAN_DATA, \"event_{}_labels_train_helix42.csv\".format(event_id))).label.values\n",
    "    return generate_dbscan_tracks(helix1, hits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_one_test_event_data(event_id, path):\n",
    "    hits  = pd.read_csv(os.path.join(path, 'event%s-hits.csv' %event_id))\n",
    "    cells = pd.read_csv(os.path.join(path, 'event%s-cells.csv'%event_id))\n",
    "    return hits\n",
    "\n",
    "def generate_df(df, filter=True):\n",
    "    df = df.copy()\n",
    "    if filter is True:\n",
    "        df = df.loc[ (df.x>0) & (df.y>0) & (df.z>0) ]\n",
    "        #df = df.loc[ (df.x>0) & (df.y>0) ]\n",
    "        \n",
    "    df = df.assign(r   = np.sqrt( df.x**2 + df.y**2))\n",
    "    df = df.assign(a   = np.arctan2(df.y, df.x))\n",
    "    df = df.assign(z1 = df.z/df.r)\n",
    "    # Other possible features that can be evaluated?\n",
    "    #df = df.assign(d   = np.sqrt( df.x**2 + df.y**2 + df.z**2 ))\n",
    "    #df = df.assign(cosa= np.cos(df.a))\n",
    "    #df = df.assign(sina= np.sin(df.a))\n",
    "    #df = df.assign(phi = np.arctan2(df.z, df.r))\n",
    "    #df = df.assign(z2 = df.z/df.d)\n",
    "    #df = df.assign(z3 = np.log1p(np.absolute(df.z/df.r))*np.sign(df.z))\n",
    "    #df = df.assign(xr = df.x/df.r)\n",
    "    #df = df.assign(yr = df.y/df.r)\n",
    "    #df = df.assign(xd = df.x/df.d)\n",
    "    #df = df.assign(yd = df.y/df.d)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_one_event_data(event_id, path=TRAIN_DATA):\n",
    "    # Only hits and truth data are currently used. May need to merge\n",
    "    # with the particles or cells data if you want to use those as features.\n",
    "    #particles = pd.read_csv(os.path.join(path, 'event%s-particles.csv'%event_id))\n",
    "    hits  = pd.read_csv(os.path.join(path, 'event%s-hits.csv' %event_id))\n",
    "    truth = pd.read_csv(os.path.join(path, 'event%s-truth.csv'%event_id))\n",
    "    #cells = pd.read_csv(os.path.join(path, 'event%s-cells.csv'%event_id))\n",
    "    \n",
    "    truth = truth.merge(hits,       on=['hit_id'],      how='left')\n",
    "    #truth = truth.merge(particles,  on=['particle_id'], how='left')\n",
    "    #truth = truth.merge(cells,      on=['hit_id'],      how='left')\n",
    "\n",
    "    #--------------------------------------------------------\n",
    "    df = truth\n",
    "    return (df)\n",
    "\n",
    "#TODO x,y,z<0\n",
    "def generate_train_batch(df):\n",
    "    df = generate_df(df)\n",
    "\n",
    "    a,r,z,z1 = df[['a', 'r', 'z', 'z1' ]].values.astype(np.float32).T\n",
    "    p = df['particle_id'].values.astype(np.int64)\n",
    "    \n",
    "    particle_ids = list(df.particle_id.unique())\n",
    "    num_particle_ids = len(particle_ids)\n",
    "\n",
    "    # Normalize inputs to a range from 0..1\n",
    "    incols  = np.column_stack((a,r/1000, z/3000,z1/3))\n",
    "    \n",
    "    tracks = []\n",
    "    \n",
    "    for particle_id in particle_ids:\n",
    "        if particle_id==0: continue\n",
    "        t = np.where(p==particle_id)[0]\n",
    "        #t = t[np.argsort(r[t])]\n",
    "        # May need to sort by abs_z?\n",
    "        t = t[np.argsort(z[t])]\n",
    "\n",
    "        # For simplicity for now, only train tracks >= 10 hits,\n",
    "        # and truncate to exactly 10 hits.\n",
    "        if len(t)<10: continue\n",
    "        track = incols[t[:10]]\n",
    "        tracks.append(track)\n",
    "\n",
    "    tracks = np.array(tracks)\n",
    "    incols = tracks[:,:5,:]\n",
    "    incols = np.pad(incols, ((0,0),(0,5), (0,0)), 'constant')\n",
    "   \n",
    "    truth  = tracks[:,:,:]\n",
    "\n",
    "    return (df, incols, truth)\n",
    "\n",
    "\n",
    "def batch_generator(invals, truth, batch_size):\n",
    "    # Create empty arrays to contain batch of features and labels#\n",
    "    #batch_features = np.zeros((batch_size, invals.shape[1], invals.shape[2]))\n",
    "    #batch_labels =  np.zeros((batch_size, truth.shape[1], truth.shape[2]))\n",
    "    index = 0\n",
    "   \n",
    "    while True:\n",
    "        batch_features = invals[batch_size*index:batch_size*(index+1),:,:]\n",
    "        batch_labels = truth[batch_size*index:batch_size*(index+1),:,:]\n",
    "        index = index+1\n",
    "        if batch_size*index > invals.shape[0]:\n",
    "            index=0\n",
    "      \n",
    "        yield batch_features, batch_labels\n",
    "\n",
    "def generate_multiple_event_data(skip=0, nevents=10):\n",
    "    start = 1000\n",
    "    invals = None\n",
    "    truth = None\n",
    "    for i in range(nevents):\n",
    "        try:\n",
    "            df = load_one_event_data('00000' + \"{:04}\".format(start+skip+i))\n",
    "            df, input_tmp, truth_tmp = generate_train_batch(df)\n",
    "            if i==0:\n",
    "                invals = input_tmp\n",
    "                truth = truth_tmp\n",
    "            else:\n",
    "                invals = np.concatenate([invals, input_tmp])\n",
    "                truth = np.concatenate([truth, truth_tmp])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return invals, truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_generate_multiple_event_data(skip=0, nevents=10):\n",
    "    print(\"Generating or loading x_train, y_train with skip {:04} and nevents {:04}\".format(skip, nevents))\n",
    "    x_train_file = os.path.join(TRAIN_NUMPY, \"event_skip_{:04}_nevents_{:04}\".format(skip, nevents)+'_x_train.npy')\n",
    "    y_train_file = os.path.join(TRAIN_NUMPY, \"event_skip_{:04}_nevents_{:04}\".format(skip, nevents)+'_y_train.npy')\n",
    "\n",
    "    if os.path.exists(x_train_file):\n",
    "        x_train = np.load(x_train_file)\n",
    "        y_train = np.load(y_train_file)\n",
    "    else:\n",
    "        # This may take 1+ hour, once it's finished, save processed events as a numpy\n",
    "        # array, which can be loaded in a few milliseconds\n",
    "        print('Generating event data, this will take a long time...')\n",
    "        x_train, y_train = generate_multiple_event_data(skip=skip, nevents=nevents)\n",
    "        np.save(x_train_file, x_train)\n",
    "        np.save(y_train_file, y_train)\n",
    "\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_batch_for_fitting(df):\n",
    "    df = generate_df(df)\n",
    "    df['track_id'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "    df['predict_done'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "    df['actual_particle_id'] = pd.Series([0] * len(df.index), index=df.index)\n",
    "\n",
    "    a, r, z, z1, hit, pid, pdone, pactual = df[['a', 'r', 'z', 'z1', 'hit_id', 'particle_id', 'predict_done', 'actual_particle_id' ]].values.astype(np.float32).T\n",
    "    p = df['particle_id'].values.astype(np.int64)\n",
    "    \n",
    "    particle_ids = list(df.particle_id.unique())\n",
    "    num_particle_ids = len(particle_ids)\n",
    "\n",
    "    incols  = np.column_stack((a,r/1000, z/3000,z1/3, hit, pid, pdone, pactual))    \n",
    "    tracks = []\n",
    "    \n",
    "    for particle_id in particle_ids:\n",
    "        if particle_id==0: continue\n",
    "        t = np.where(p==particle_id)[0]\n",
    "        t = t[np.argsort(z[t])]\n",
    "\n",
    "        if len(t)<10: continue\n",
    "        track = incols[t[:10]]\n",
    "        track[:5,7] = particle_id\n",
    "        tracks.append(track)\n",
    "\n",
    "    tracks = np.array(tracks)\n",
    "    incols = np.copy(tracks[:,:,:])\n",
    "    incols[:,5:,0:4] = 0\n",
    "   \n",
    "    truth  = tracks[:,:,:]\n",
    "\n",
    "    return (df, incols, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predictions(hits, preds, verbose=False):\n",
    "    \"\"\"\n",
    "    Assign predicted LSTM hits to nearest un-assigned hit.\n",
    "    \"\"\"\n",
    "    def do_one_assignment_round(hits, preds):\n",
    "        labels = hits.track_id.values\n",
    "        distances = np.zeros((len(labels)), dtype=float)\n",
    "        predi = np.zeros((len(labels)), dtype=int)\n",
    "        predj = np.zeros((len(labels)), dtype=int)\n",
    "        #count_free_hits = len(np.where(labels == 0)[0])\n",
    "        #print('free hits available this round: ' + str(count_free_hits))\n",
    "\n",
    "        # Backup current predictions to restore a/rn/zn/zrn values if\n",
    "        # another hit ends up being closer\n",
    "        orig_preds = np.copy(preds)\n",
    "\n",
    "        # Note - df is the list of all available hits not yet assigned to\n",
    "        # a track. We need to fit our predictions to these hits.\n",
    "        df = hits.loc[(hits.track_id == 0)]\n",
    "        hit_ids = df.hit_id.values\n",
    "\n",
    "        # We have a, r/1000, z/3000, and (z/r)/3\n",
    "        a, rn, zn, zrn, pid = df[['a', 'rn', 'zn', 'zrn', 'particle_id']].values.T\n",
    "        tree = KDTree(np.column_stack([a, rn, zn, zrn]), metric='euclidean')\n",
    "\n",
    "        num_left_to_assign = 0\n",
    "        # For each predicted track\n",
    "        for i in range(preds.shape[0]):\n",
    "            # For each predicted hit in that track\n",
    "            truth_particle = preds[i,0,5]\n",
    "            for j in range(preds.shape[1]):\n",
    "                # hits 0-4 are the input seeds, already assigned\n",
    "                if j < 5: continue\n",
    "                # If we've already assigned this prediction to a hit in\n",
    "                # a previous round, skip it.\n",
    "                if preds[i,j,6] != 0: continue\n",
    "\n",
    "                # Find nearest neighbour to LSTM-predicted hits\n",
    "                ga = preds[i,j,0]\n",
    "                grn = preds[i,j,1]\n",
    "                gzn = preds[i,j,2]\n",
    "                gzrn = preds[i,j,3]\n",
    "                (nearest_dist, nearest_idx) = tree.query([[ga, grn, gzn, gzrn]], k=1)\n",
    "                nearest_dist = np.concatenate(nearest_dist)\n",
    "                nd0 = nearest_dist[0]\n",
    "                nearest_idx = np.concatenate(nearest_idx)\n",
    "                nidx0 = nearest_idx[0]\n",
    "                hit_id = hit_ids[nidx0]\n",
    "                gidx0 = hit_id - 1\n",
    "\n",
    "                #print('end point 0: ' + str(gzr0))\n",
    "                #print('free idx: ' + str(nidx0))\n",
    "                #print('nearest point: ' + str(zr[nidx0]))\n",
    "                #print('nearest distance: ' + str(nd0))\n",
    "                #print('global index: ' + str(gidx0))\n",
    "                #print('global zr: ' + str(gzr[gidx0]))\n",
    "\n",
    "                hits_index = np.where(hits['hit_id'] == hit_id)[0]\n",
    "                if (labels[hits_index] == 0) or (nd0 < distances[hits_index]):\n",
    "                    if (labels[hits_index] != 0):\n",
    "                        # We stole someone else's prediction since we are closer to\n",
    "                        # that hit, so clear the 'prediction_done' flag\n",
    "                        far_i = predi[hits_index]\n",
    "                        far_j = predj[hits_index]\n",
    "                        preds[far_i,far_j,:] = orig_preds[far_i,far_j,:]\n",
    "                        num_left_to_assign = num_left_to_assign + 1\n",
    "                    # Tentatively assign this hit\n",
    "                    preds[i,j,6] = 1\n",
    "                    # And record what the actual particle/track ID was\n",
    "                    preds[i,j,7] = pid[nidx0]\n",
    "                    # And remember the fitted a/rn/zn/zrn values so we can visualize\n",
    "                    preds[i,j,0] = a[nidx0]\n",
    "                    preds[i,j,1] = rn[nidx0]\n",
    "                    preds[i,j,2] = zn[nidx0]\n",
    "                    preds[i,j,3] = zrn[nidx0]\n",
    "\n",
    "                    # And record our track and the distance between the projected hit\n",
    "                    # and the actual hit. This distance is used in case a different\n",
    "                    # projected hit has this same target as its nearest neighbour.\n",
    "                    # The target hit will be assigned to the closest one.\n",
    "                    labels[hits_index] = truth_particle\n",
    "                    distances[hits_index] = nd0\n",
    "                    predi[hits_index] = i\n",
    "                    predj[hits_index] = j\n",
    "                elif distances[hits_index] > 0:\n",
    "                    num_left_to_assign = num_left_to_assign + 1\n",
    "                else:\n",
    "                    num_left_to_assign = num_left_to_assign + 1\n",
    "\n",
    "        hits['track_id'] = labels.tolist()\n",
    "        return (hits, preds, num_left_to_assign)\n",
    "\n",
    "    #hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['rn'] = hits['r'] / 1000\n",
    "    hits['a'] = np.arctan2(hits.y.values, hits.x.values)\n",
    "    hits['zr'] = hits['z'] / hits['r']\n",
    "    hits['zrn'] = hits['zr'] / 3\n",
    "    hits['zn'] = hits['z'] / 3000\n",
    "\n",
    "\n",
    "    # Initialize tracks using our initial 5 seed hits, and using the\n",
    "    # truth particle ID as the track ID\n",
    "    labels = hits.track_id.values\n",
    "    for i in range(preds.shape[0]):\n",
    "        truth_particle_id = preds[i,0,5]\n",
    "        # First 5 hits are known, update track_id with ground truth particle ID\n",
    "        for j in range(5):\n",
    "            hit_id = preds[i,j,4]\n",
    "            hits_index = np.where(hits['hit_id'] == hit_id)[0]\n",
    "            labels[hits_index] = truth_particle_id\n",
    "    hits['track_id'] = labels.tolist()\n",
    "\n",
    "    num_left_to_assign = preds.shape[0]*5 # 5 hits to assign per predicted track\n",
    "    num_loops = 0\n",
    "    while (num_left_to_assign > 0) and (num_loops < 10):\n",
    "        (hits, preds, num_left_to_assign) = do_one_assignment_round(hits, preds)\n",
    "        num_loops = num_loops + 1\n",
    "        if verbose:\n",
    "            print(\"Num left to assign after round {}: {}\".format(num_loops, num_left_to_assign))\n",
    "\n",
    "    return (hits, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fit_accuracy(preds, verbose=False):\n",
    "    total_correct = 0\n",
    "    total_incorrect = 0\n",
    "    tracks_right = np.zeros((6), dtype=int)\n",
    "    perfect_tracks = []\n",
    "    seeds_right = np.zeros((6), dtype=int)\n",
    "    perfect_seeds = []\n",
    "    total_seed_correct = 0\n",
    "    total_seed_incorrect = 0\n",
    "    for i in range(preds.shape[0]):\n",
    "        track_right = 0\n",
    "        seed_right = 0\n",
    "        for j in range(5):\n",
    "            #print('truth: ' + str(preds[i][j][5]))\n",
    "            #print('fit:   ' + str(preds[i][j][7]))\n",
    "            if preds[i][j][5] != preds[i][j][7]:\n",
    "                total_seed_incorrect = total_seed_incorrect + 1\n",
    "            else:\n",
    "                total_seed_correct = total_seed_correct + 1\n",
    "                seed_right = seed_right + 1\n",
    "        seeds_right[seed_right] = seeds_right[seed_right] + 1\n",
    "        if seed_right == 5:\n",
    "            perfect_seeds.append(i)\n",
    "\n",
    "        for j in range(5, 10):\n",
    "            #print('truth: ' + str(preds[i][j][5]))\n",
    "            #print('fit:   ' + str(preds[i][j][7]))\n",
    "            if preds[i][j][5] != preds[i][j][7]:\n",
    "                total_incorrect = total_incorrect + 1\n",
    "            else:\n",
    "                total_correct = total_correct + 1\n",
    "                track_right = track_right + 1\n",
    "        tracks_right[track_right] = tracks_right[track_right] + 1\n",
    "        if track_right == 5:\n",
    "            perfect_tracks.append(i)\n",
    "\n",
    "    accuracy = total_correct / (total_correct + total_incorrect)\n",
    "    seed_accuracy = total_seed_correct / (total_seed_correct + total_seed_incorrect)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Total correct:   {}\".format(total_correct))\n",
    "        print(\"Total incorrect: {}\".format(total_incorrect))\n",
    "        print(\"Total accuracy:  {}\".format(accuracy))\n",
    "        print(\"Total correct predictions per track (0-5): {}\".format(tracks_right))\n",
    "\n",
    "        print(\"Total Seed correct:   {}\".format(total_seed_correct))\n",
    "        print(\"Total Seed incorrect: {}\".format(total_seed_incorrect))\n",
    "        print(\"Total Seed accuracy:  {}\".format(seed_accuracy))\n",
    "        print(\"Total Seed per track (0-5): {}\".format(seeds_right))\n",
    "\n",
    "    return accuracy, tracks_right, seed_accuracy, seeds_right\n",
    "    #print(perfect_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions(model_names, dbscan=False, first_event=9998, num_events=1, verbose=False):\n",
    "    start = time.time()\n",
    "    gpu_models = []\n",
    "    for i in range(len(model_names)):\n",
    "        _, gpu_modeli = load_existing_model(model_names[i])\n",
    "        gpu_models.append(gpu_modeli)\n",
    "    num_models = len(gpu_models)\n",
    "    elapsed = find_elapsed_time(start, \"Model load time ({} models): \".format(num_models))\n",
    "\n",
    "    avg_accuracy = 0.0\n",
    "    track_dists = np.zeros((6), dtype=int)\n",
    "    avg_seed_accuracy = 0.0\n",
    "    seed_dists = np.zeros((6), dtype=int)\n",
    "    \n",
    "    for ev in range(num_events):\n",
    "        start = time.time()\n",
    "        event_id = first_event + ev\n",
    "        fit_df = load_one_event_data(\"00000{}\".format(event_id))\n",
    "        if dbscan:\n",
    "            fit_df, pred_x, pred_truth = generate_dbscan_test_data(event_id, fit_df)\n",
    "        else:\n",
    "            fit_df, pred_x, pred_truth = generate_train_batch_for_fitting(fit_df)\n",
    "        num_tracks = pred_truth.shape[0]\n",
    "        elapsed = find_elapsed_time(start, \"Event {} with {} tracks setup time: \".format(event_id, num_tracks))\n",
    "\n",
    "        preds = np.copy(pred_x)\n",
    "        preds[:,5:,0:4] = 0\n",
    "        elapsed = 0\n",
    "        for i in range(num_models):\n",
    "            start = time.time()\n",
    "            pred_raw = gpu_models[i].predict(pred_x[:,:,0:4])\n",
    "            elapsed = elapsed + find_elapsed_time(start, \"predict time: \", display_time=verbose)\n",
    "            preds[:,5:,0:4] = preds[:,5:,0:4] + pred_raw[:,5:,0:4]\n",
    "\n",
    "            if verbose:\n",
    "                test_loss, test_acc, test_mae, test_mape = gpu_models[i].evaluate(pred_x[:,:,0:4], pred_truth[:,:,0:4])\n",
    "                print(\"Loss: MSE: {:f}, MAE: {:f}, MAPE: {:f}\".format(test_loss, test_mae, test_mape))\n",
    "\n",
    "        print(\"Event {} predict time: {:f} (per track: {:f})\".format(event_id, elapsed, elapsed/num_tracks))\n",
    "\n",
    "        preds[:,5:,0:4] = preds[:,5:,0:4] / num_models\n",
    "\n",
    "        # Set up predictions for kNN fitting - we only fit predictions 5-10.\n",
    "        start = time.time()\n",
    "        (fit_df, preds) = fit_predictions(fit_df, preds)\n",
    "        elapsed = find_elapsed_time(start, \"x\", display_time=False)\n",
    "        print(\"Event {} fit time: {:f} (per track: {:f})\".format(event_id, elapsed, elapsed/num_tracks))\n",
    "\n",
    "        (accuracy, dist, seed_accuracy, seed_dist) = calculate_fit_accuracy(preds, verbose=verbose)\n",
    "        print(\"Event {} accuracy: {:f}, distribution (0-5 correct): {}\".format(event_id, accuracy, dist))\n",
    "        print(\"Event {} seed accuracy: {:f}, distribution (0-5 correct): {}\".format(event_id, seed_accuracy, seed_dist))\n",
    "        avg_accuracy = avg_accuracy + accuracy\n",
    "        avg_seed_accuracy = avg_seed_accuracy + seed_accuracy\n",
    "        track_dists = track_dists + dist\n",
    "        seed_dists = seed_dists + seed_dist\n",
    "    \n",
    "        if VISUALIZE_RESULTS:\n",
    "            draw_prediction(preds_truth[:,:,0:4], preds[:,:,0:4], 270, 280)\n",
    "\n",
    "    if num_events > 1:\n",
    "        avg_accuracy = avg_accuracy / num_events\n",
    "        avg_seed_accuracy = avg_seed_accuracy / num_events\n",
    "        print(\"Average accuracy: {:f}, total distribution (0-5 correct): {}\".format(avg_accuracy, track_dists))\n",
    "        print(\"Average seed accuracy: {:f}, total seed distribution (0-5 correct): {}\".format(avg_seed_accuracy, seed_dists))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_elapsed_time(start, label, display_time=True):\n",
    "    elapsed = time.time()\n",
    "    elapsed = elapsed - start\n",
    "    if display_time:\n",
    "        print(\"{}{:f}\".format(label, elapsed))\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # For playing around just load a couple hundred events, for final LSTM\n",
    "    # training, all possible events should be used to minimize loss.\n",
    "    train_skip = 0\n",
    "    train_nevents = 200\n",
    "    val_skip = 0\n",
    "    val_nevents = 20\n",
    "    print('loading train data...')\n",
    "    invals, truth = load_or_generate_multiple_event_data(skip=train_skip, nevents=train_nevents)\n",
    "    print('Done!')\n",
    "\n",
    "    print('loading val data...')\n",
    "    val_input, val_truth = load_or_generate_multiple_event_data(skip=train_skip+train_nevents+val_skip, nevents=val_nevents)\n",
    "    print('Done!')\n",
    "\n",
    "    if LOAD_MODEL_NAME is not None:\n",
    "        model, gpu_model = load_existing_model(LOAD_MODEL_NAME)\n",
    "    else:\n",
    "        model, gpu_model = build_new_model(input_shape=(invals.shape[1],invals.shape[2]), output_shape=(invals.shape[1],invals.shape[2]))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    batch_size = 4096\n",
    "    num_epoch = 100\n",
    "    #generator = batch_generator(invals, truth, batch_size)\n",
    "    #val_generator = batch_generator(val_input, val_truth, batch_size)\n",
    "    # Train the model\n",
    "    #history = gpu_model.fit_generator(generator, validation_data=val_generator,validation_steps=int(val_input.shape[0]/batch_size),\n",
    "    #                              steps_per_epoch=int(invals.shape[0]/batch_size), epochs=num_epoch, \n",
    "    #                              shuffle = False)\n",
    "    history = gpu_model.fit(x=invals, y=truth, validation_data=(val_input, val_truth), batch_size=batch_size,\n",
    "                            epochs=num_epoch, shuffle=False)\n",
    "\n",
    "    current_datetime_str = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    save_model_name = current_datetime_str + '.h5'\n",
    "    model.save(save_model_name)\n",
    "    predict_model_names.append(save_model_name)\n",
    "    print(\"End time: {}\".format(current_datetime_str))\n",
    "\n",
    "    if VISUALIZE_RESULTS:\n",
    "        draw_train_history(history, metric='mean_squared_error', metric_ylabel='Mean Squared Err', metric_title='Mean Squared Error', draw_val=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/trackml/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model load time (2 models): 8.087303\n",
      "Event 9998 with 646 tracks setup time: 0.457160\n",
      "Event 9998 predict time: 4.661150 (per track: 0.007215)\n",
      "Event 9998 fit time: 1.256552 (per track: 0.001945)\n",
      "Event 9998 accuracy: 0.859752, distribution (0-5 correct): [  3  13  27  75 155 373]\n",
      "Event 9998 seed accuracy: 1.000000, distribution (0-5 correct): [  0   0   0   0   0 646]\n",
      "Event 9999 with 612 tracks setup time: 0.369602\n",
      "Event 9999 predict time: 4.638943 (per track: 0.007580)\n",
      "Event 9999 fit time: 1.119281 (per track: 0.001829)\n",
      "Event 9999 accuracy: 0.852288, distribution (0-5 correct): [  2  15  33  70 143 349]\n",
      "Event 9999 seed accuracy: 1.000000, distribution (0-5 correct): [  0   0   0   0   0 612]\n",
      "Average accuracy: 0.856020, total distribution (0-5 correct): [  5  28  60 145 298 722]\n",
      "Average seed accuracy: 1.000000, total seed distribution (0-5 correct): [   0    0    0    0    0 1258]\n"
     ]
    }
   ],
   "source": [
    "#predict_model_names = ['1024-epoch-300-mae.h5', '512-512-200-epoch.h5']#'512-02-512-epoch-100.h5']#, '1024-epoch-200.h5']\n",
    "#predict_model_names = ['1024-epoch-400-mae.h5', '1024-epoch-300-mae.h5'] # 84.6% accuracy, 72.6% dbscan\n",
    "#predict_model_names = ['1024-epoch-400-mae.h5', '2018-11-18-14-13-59.h5']\n",
    "#predict_model_names = ['1024-epoch-300-mae.h5', '1024-epoch-200-mae.h5'] # 85.6% accuracy! 15 step, 79.1% dbscan accuracy\n",
    "## NEW DBSCAN: Restrict truth > 10 hits where x,y,z > 0\n",
    "## - 452 tracks to predict, 82.7% fit accuracy ([ 10  13  26  62  88 253]), 95.6% seeds ([  0   1   4  20  43 384])\n",
    "#predict_model_names = ['1024-epoch-300-mae.h5', '1024-epoch-200-mae.h5']\n",
    "#predict_model_names = ['2048-epoch-400-mae.h5', '1024-epoch-400-mae.h5']\n",
    "# truth: 646 tracks, 86.0% ([  3  13  27  75 155 373])\n",
    "# dbscan: 452 tracks, 82.9% ([  9  11  35  51  90 256]), seeds 95.6% ([  0   1   4  20  43 384])\n",
    "# dbscan 100: Average accuracy: 0.803291, total distribution (0-5 correct): [ 1574  1580  3264  5182  8920 23300]\n",
    "# dbscan 100: Average seed accuracy: 0.957257, total seed distribution (0-5 correct): [    0    26   777  1619  3787 37611]\n",
    "# truth 100:  Average accuracy: 0.845796, total distribution (0-5 correct): [  190  1290  3792  8004 15838 34696]\n",
    "ensemble_predictions(predict_model_names, dbscan=False, first_event=9900, num_events=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
